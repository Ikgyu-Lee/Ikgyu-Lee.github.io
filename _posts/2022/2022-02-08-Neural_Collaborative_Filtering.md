---
title: Neural Collaborative Filtering
layout: post
Created: January 1, 2022 6:28 PM
tags:
    - Paper
    - Recommendation System
use_math: true

---



> ğŸ§  2017ë…„ WWWì— ë°œí‘œëœ 'Neural Collaborative Filtering' ë…¼ë¬¸ ìš”ì•½ ì •ë¦¬ ë° ì½”ë“œ êµ¬í˜„ì…ë‹ˆë‹¤.
> [Neural Collaborative Filtering Paper](https://arxiv.org/abs/1708.05031)
> [NCF Repository](https://github.com/IkGyu-Lee/NCF)



# 1. Abstract & Introduction

> NCF(Neural Collaborative Filtering) : replacing the inner product with a neural architecture that can learn an arbitrary function from data
>
>
> To supercharge NCF modelling with non-linearities, we propose to leverage a multi-layer perceptron to learn the userâ€“item interaction function


- MFì˜ ë‚´ì ì„ ëŒ€ì²´í•˜ëŠ” ìš©ë„ë¡œ Neural Architectureì„ ì œì•ˆí•œë‹¤.
- superchargeì„ ìœ„í•´ MLPë¥¼ í™œìš©í•œë‹¤.

> Despite the effectiveness of MF for collaborative filtering, it is well-known that its performance can be hindered by the simple choice of the interaction function â€” inner product.
>
>
> The inner product, which simply combines the multiplication of latent features linearly, may not be sufficient to capture the complex structure of user interaction data.


- MFë¥¼ ê°œì„ ì‹œí‚¤ê¸° ìœ„í•´ ë‹¤ì–‘í•œ ë…¼ë¬¸ë“¤ì´ ë‚˜ì™”ì§€ë§Œ, MFê°€ ì‚¬ìš©í•˜ëŠ” ë‚´ì ì˜ íŠ¹ì„±ìƒ ë‹¨ìˆœí•œ ì„ íƒìœ¼ë¡œ ì„±ëŠ¥ì„ ì €í•´ì‹œí‚¤ë©°, ë³µì¡í•œ êµ¬ì¡°ì— ì¶©ì¡±ë˜ê¸° ì–´ë µë‹¤.

> We focus on [implicit feedback](https://www.notion.so/Explicit-vs-Implicit-Feedback-Data-9b2eac5db6ee442ba75d81e17fd47828)


- í•´ë‹¹ ë…¼ë¬¸ì€ userê°€ ì§ì ‘(explicit) í‰ê°€í•˜ëŠ” dataê°€ ì•„ë‹Œ, implicit dataì— ì¤‘ì ì„ ë‘”ë‹¤.

> We present a neural network architecture to model latent features of users and items and devise a general framework NCF for collaborative filtering based on neural networks.
>
>
> We show that MF can be interpreted as a specialization of NCF and utilize a multi-layer perceptron to endow NCF modelling with a high level of non-linearities.
>
> We perform extensive experiments on two real-world datasets to demonstrate the effectiveness of our NCF approaches and the promise of deep learning for collaborative filtering.

- 3ê°€ì§€ main contributions

# 2. Preliminaries

## 2.1 Learning from Implicit Data
<div class="center">
  <figure>
    <a href="/images/2022/NCF/t0.png"><img src="/images/2022/NCF/t0.png" width="600"  ></a>
  </figure>
</div>

- user's implicit feedbackì´ê¸° ë•Œë¬¸ì— explicit preferenceë¥¼ ë‚˜íƒ€ë‚´ì§€ ì•ŠëŠ”ë‹¤. ë”°ë¼ì„œ $y_{u,i} = 0$ ì€ userì™€ itemê°„ì˜ interactionì´ ì—†ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.

> Moving one step forward, our NCF framework parameterizes the interaction function f using neural networks to estimate Ë†yui. As such, it naturally supports both pointwise and pairwise learning.

- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” NCFì˜ $\theta$ë¥¼ í•™ìŠµí•˜ëŠ”ë° [pointwise learning, pairwise learning](https://www.notion.so/Pointwise-vs-Pairwise-vs-Listwise-8661583de4f7418fb1914f96d2b66250) 2ê°€ì§€ ë°©ë²• ëª¨ë‘ ë‹¤ ì‚¬ìš©í•˜ê³ ì í•œë‹¤.

## 2.2 Matrix Factorization

<div class="center">
  <figure>
    <a href="/images/2022/NCF/t1.png"><img src="/images/2022/NCF/t1.png" width="600"  ></a>
  </figure>
</div>

- $p_u$ : user latent vector
- $q_i$ : item latent vector

> MF models the two-way interaction of user and item latent factors, assuming each dimension of the latent space is independent of each other and linearly combining them with the same weight.

- MFëŠ” **user latent vector**ì™€ **item latent vector**ì˜ inner productë¥¼ í†µí•´ interactionì„ modelingí•œë‹¤. ê° latent spaceëŠ” ì„œë¡œ ë…ë¦½ì ì´ë©°, ê°™ì€ weightë¡œ linearly combiningí•œë‹¤.

### Matrix Factorization's limit

<div class="center">
  <figure>
    <a href="/images/2022/NCF/t2.png"><img src="/images/2022/NCF/t2.png" width="600"  ></a>
  </figure>
</div>

- ë³¸ ë…¼ë¬¸ì—ì„œëŠ” [Jaccard coefficient](https://www.notion.so/Similarity-28496d0e6cbd4f70b3ce053cc2b08e76)ë¥¼ í™œìš©í•˜ì—¬ MFì˜ í•œê³„ë¥¼ ì„¤ëª…í•œë‹¤.

ë¨¼ì €, user 1,2,3ë§Œ ê³ ë ¤í–ˆì„ ë•Œì˜ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$s_{23}(0.66) > s_{12}(0.5) > s_{13}(0.4)$

ì´ì— ë”°ë¼ latent spaceì— $p_1$, $p_2$, $p_3$ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤.

ë‹¤ìŒìœ¼ë¡œ, user 4ë¥¼ í•¨ê»˜ ê³ ë ¤í•˜ê²Œ ë˜ì—ˆì„ ë•Œì˜ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

$s_{41}(0.6) > s_{43}(0.4) > s_{42}(0.2)$

user 4ë¥¼ user 1ê³¼ ê°€ì¥ ìœ ì‚¬í•˜ë©´ì„œ user 3ë³´ë‹¤ user 2ê°€ ëœ ìœ ì‚¬í•œ $p_4$ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ì—†ë‹¤. ì´ê²ƒì€ ì¦‰, MFì˜ inner product í•œê³„ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ì´ëŠ” userì™€ item ì‚¬ì´ì˜ complex interactionì„ low dimensional latent spaceë¡œ ë‚˜íƒ€ëƒˆê¸° ë•Œë¬¸ì´ë‹¤.

> We note that one way to resolve the issue is to use a large number of latent factors K. However, it may adversely hurt the generalization of the model (e.g., overfitting the data), especially in sparse settings

- ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ latent factorì˜ dimensionì„ ë†’ì¼ ìˆ˜ ìˆì§€ë§Œ, ì´ëŠ” generalization of the modelì„ ì €í•´í•˜ê²Œ ëœë‹¤. ì¦‰, ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆë‹¤.
- ë”°ë¼ì„œ ë³¸ ë…¼ë¬¸ì˜ ì €ìëŠ” DNNì„ ì´ìš©í•´ í•´ë‹¹ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³ ì í•œë‹¤.

# 3. Neural Collaborative Filtering

user-item interaction functionì„ í•™ìŠµí•˜ê¸° ìœ„í•´ NCFëŠ” non-linearity(ë¹„ì„ í˜•ì„±)ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” MLPë¥¼ ì‚¬ìš©í•œë‹¤.

## 3.1 General Framework

<div class="center">
  <figure>
    <a href="/images/2022/NCF/t3.png"><img src="/images/2022/NCF/t3.png" width="600"  ></a>
  </figure>
</div>
- Input Layer (Sparse)

inputìœ¼ë¡œ ê°ê° userì™€ itemì˜ one-hot encodingì„ í•œ vectorë¥¼ ì‚¬ìš©í•œë‹¤. ì´ëŠ” binarized sparse vectorì´ë‹¤.

- Embedding Layer

inputì˜ sparse vectorë¥¼ dense vectorë¡œ mappingí•œë‹¤. ì¼ë°˜ì ì¸ embedding ë°©ë²•ê³¼ ë™ì¼í•˜ê²Œ fully-connected layerë¥¼ ì‚¬ìš©í•œë‹¤.

- Neural CF Layers

user latent vectorì™€ item latent vectorë¥¼ concatenationí•œ vectorë¥¼ inputìœ¼ë¡œ DNNì„ ê±°ì¹œë‹¤. ë§ˆì§€ë§‰ hidden layer Xì˜ dimensionì´ ëª¨ë¸ì˜ capabilityë¥¼ ê²°ì •í•œë‹¤. ì´ë¥¼ í†µí•´ non-linearity dataë¥¼ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

- Output Layer

Score $\hat{y}_{ui}$ë¥¼ ì˜ˆì¸¡í•œë‹¤.

- Training

point-wise lossì™€ pair-wise loss(Bayesian Personalized Ranking, margin-based loss) ë‘˜ ë‹¤ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‚˜, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” $\hat{y}_{ui}$ê³¼ target valueì¸ $y_{ui}$ì‚¬ì´ë¥¼ ìµœì†Œí™”í•˜ëŠ” point-wise lossë§Œ ì‚¬ìš©í•˜ì—¬ í•™ìŠµí•œë‹¤.

- NCF model

<div class="center">
  <figure>
    <a href="/images/2022/NCF/t4.png"><img src="/images/2022/NCF/t4.png" width="600"  ></a>
  </figure>
</div>
$P$ì™€ $Q$ëŠ” embedding layerì˜ Matrix $P\in \mathbb R^{M\times K}, Q\in \mathbb R^{N\times K}$ì´ê³ , $\theta_{f}$ëŠ” interaction function $f$ì˜ model parameterì´ë‹¤. function $f$ì€ multi-layer neural networkì´ê¸°ì— í‘œê¸°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

![Untitled](/images/2022/NCF/t5.png)

### Learning NCF

implicit dataì´ê¸°ì— binaryí•œ íŠ¹ì§•(bernoulli distribution)ì„ ê³ ë ¤í•˜ì—¬ Logisticì´ë‚˜ Probit functionì„ ì‚¬ìš©í•˜ì—¬, $\hat{y}_{ui}$ì„ [0, 1]ì˜ ë²”ìœ„ë¥¼ ê°€ì§€ê²Œ í•œë‹¤. ì´ì— ë”°ë¥¸ likelihoodëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.

<div class="center">
  <figure>
    <a href="/images/2022/NCF/t6.png"><img src="/images/2022/NCF/t6.png" width="600"  ></a>
  </figure>
</div>
ì´ì–´ loss functionì€ ì•„ë˜ì™€ ê°™ìœ¼ë©°, binary cross-entropy lossë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. í•´ë‹¹ modelì€ $L$ì˜ ê°’ì„ ìµœì†Œí™”í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ê²Œ ëœë‹¤.

![Untitled](/images/2022/NCF/t7.png)

optimizerë¡œëŠ” SGDë¥¼ ì‚¬ìš©í•˜ë©°, unobserved interactionì— ëŒ€í•œ itemì„ negative sampleë¡œ ì‚¬ìš©í•˜ì˜€ë‹¤.

## 3.2 Generalized Matrix Factorization (GMF)

> MF can be interpreted as a special case of our NCF framework
>
- NCFì˜ special caseë¡œ MFë¥¼ ì ìš©ì‹œí‚¬ ìˆ˜ ìˆë‹¤.

latent vector $P^Tv^U_{u}, Q^Tv^I_{i}$ë¥¼ ê°ê° $p_u, q_i$ë¼ê³  í‘œí˜„, $a_{out}$ì€ activation functionì„ ì˜ë¯¸í•˜ë©°, $h$ëŠ” output layerì˜ ê°€ì¤‘ì¹˜ë¥¼ ì˜ë¯¸í•œë‹¤. $\odot$ì€ element-wise product(Hadamard product)ë¥¼ ì˜ë¯¸í•œë‹¤. output layerì— projectí•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

<div class="center">
  <figure>
    <a href="/images/2022/NCF/t8.png"><img src="/images/2022/NCF/t8.png" width="600"  ></a>
  </figure>
</div>
$a_{out}$ì´ indentity function, $h$ëŠ” uniform vectorë¼ë©´, ê¸°ì¡´ì˜ MFì™€ ë™ì¼í•˜ë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” $a_{out}$ì„ sigmoid function( $\sigma(x) = 1/(1+e^{-x})$ ), $h$ë¥¼ log lossë¥¼ ì‚¬ìš©í•´ GMFë¥¼ í•™ìŠµí•˜ì˜€ë‹¤.

## 3.3 Multi-Layer Perceptron (MLP)

<div class="center">
  <figure>
    <a href="/images/2022/NCF/t9.png"><img src="/images/2022/NCF/t9.png" width="600"  ></a>
  </figure>
</div>
$\phi_1$ëŠ” concatenate í•¨ìˆ˜, $W_x$ëŠ” weight matrix, $b_x$ëŠ” bias vector, $a_x$ëŠ” activation functionì„ ì˜ë¯¸í•œë‹¤.

ë‹¨ìˆœí•œ vectorì˜ concatenationìœ¼ë¡œ userì™€ item ì‚¬ì´ì˜ ìƒí˜¸ì‘ìš©ì„ ì„¤ëª…í•˜ì§€ ì•Šì•„, CF modelingì˜ íš¨ê³¼ë¥¼ ì£¼ê¸°ì—ëŠ” ì¶©ë¶„í•˜ì§€ ì•Šë‹¤. ì´ ë•Œë¬¸ì— MLPë¥¼ ì‚¬ìš©í–ˆë‹¤.

## 3.4 Fusion of GMF and MLP (NeuMF)

NCF frameworkë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ GMFì™€ MLPë¥¼ fuseí•œ ê²ƒì´ Neural Matrix Factorization(NeuMF)ì´ë‹¤.

<div class="center">
  <figure>
    <a href="/images/2022/NCF/t10.png"><img src="/images/2022/NCF/t10.png" width="600"  ></a>
  </figure>
</div>

- GMF LayerëŠ” userì™€ item vectorê°„ì˜ element-wise productë¥¼ ì§„í–‰í•˜ì—¬, latent feature interactionì„ ìœ„í•´ linear kernelì„ apply.
- MLP Layer XëŠ” userì™€ item vectorë¥¼ concatenateí•˜ì—¬ hidden layer(MLP Layer i)ë¥¼ ì§„í–‰í•œë‹¤. dataì—ì„œ interaction functionì„ í•™ìŠµí•˜ê¸° ìœ„í•´ non-linear kernelì„ apply.
- NeuMF LayerëŠ” GMFì™€ MLPì˜ outputì„ concatenateí•˜ì—¬ $\hat{y}_{ui}$(score)ë¥¼ ì˜ˆì¸¡í•œë‹¤.
- GMFì™€ MLP Layerë¥¼ ê°ê° ë¨¼ì € pre-trainingí•œ ë‹¤ìŒ, last hidden layerë¥¼ concatí•˜ì—¬ NeuMFë¥¼ í•™ìŠµ.
- GMF, MLP, NeuMF without pretrainingì€ Adam, NeuMF with pretrainingì€ SGDë¥¼ Optimizerë¡œ ì‚¬ìš©.

![Untitled](/images/2022/NCF/t11.png)

GMF, MLP ë‘˜ ë‹¤ ê°™ì€ embedding sizeë¥¼ ê°€ì ¸ì•¼í•˜ê¸° ë•Œë¬¸ì— performanceì— limitì´ ìˆì„ ìˆ˜ ìˆìœ¼ë©°, optimal embedding sizeê°€ ë§ì´ ë³€í™”í•˜ëŠ” datasetì€ optimal ensembleì„ ë‚¼ ìˆ˜ ì—†ì„ ì§€ë„ ëª¨ë¥¸ë‹¤.

### 3.4.1 Pre-training

ë‹¨ìˆœíˆ GMFì™€ MLPë¥¼ concatí•˜ì—¬ objective functionìœ¼ë¡œ Adamì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ gradient-based optimization ë°©ë²•ë“¤ì˜ íŠ¹ì„±ìƒ local optimaë¡œ ë¹ ì§ˆ ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ìˆ˜ë ´ê³¼ ì„±ëŠ¥ì— initializationì´ ì¤‘ìš”í•œ ì—­í• ì„ í•œë‹¤ê³  ì•Œë ¤ì ¸ ìˆë‹¤. ë”°ë¼ì„œ ë¬´ì‘ìœ„ì˜ initializationì„ í•˜ì—¬ NeuMFë¥¼ í•™ìŠµí•˜ëŠ” ê³¼ì •ì—ì„œ parameterë“¤ì„ updateí•˜ëŠ” ê²ƒë³´ë‹¤ convergence(ìˆ˜ë ´)ë˜ì–´ ìˆëŠ” GMFì™€ MLPì˜ parameterë“¤ì„ ê°€ì ¸ì™€ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” performaceê°€ ì¢‹ë‹¤.

# 4. Experiments

ì‹¤í—˜ì€ 3ê°€ì§€ì˜ ì§ˆë¬¸ì„ ë°”íƒ•ìœ¼ë¡œ ì§„í–‰í•œë‹¤.

Q1) Do our proposed NCF methods outperform the state-of-the-art implicit collaborative filtering methods?

- Performance comparison

Q2) How does our proposed optimization framework (log loss with negative sampling) work for the recommendation task?

- Log loss with Negative sampling

Q3) Are deeper layers of hidden units helpful for learning from user-item interaction data?

- Is deep learning helpful?

### 4.1 Experimental Settings

![Untitled](/images/2022/NCF/t12.png)

- Dataset : MovieLensì™€ Pinterest dataset ì‚¬ìš©í•˜ë©°, ìµœì†Œ 20ê°œì˜ ratingì´ë‚˜ pinì„ ë‚¨ê¸´ userì˜ dataë§Œìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•œë‹¤.
- Evaluation Protocols : Hit Ratio, NDCG
- Baselines : NCF method(GMF, MLP, NeuMF)ì™€ ë¹„êµí•˜ê¸°ìœ„í•´ baselineìœ¼ë¡œ ItemKNN, BPR, eALSë¥¼ ì‚¬ìš©í•œë‹¤.
- Parameter Settings :
    - Binary Log loss function(BCE)
    - Sampling four negative instances per positive instance
    - randomly initialized model parameters with (Gaussian Distribution, mean=0, std=0.01)
    - optimizer : Adam(GMF, MLP), SGD(NeuMF)
    - batch size : [128, 256, 512, 1024]
    - learning rate : [0.0001, 0.0005, 0.001, 0.005]
    - predictive factors and evaluated the factors : [8, 16, 32, 64]
    - NeuMF with pre-training, Î± was set to 0.5
- Ex) predictive factorê°€ 8ì´ê³ , model architectureê°€ â€œ32 â†’ 16 â†’ 8â€ì¸ modelì„ ë§Œë“ ë‹¤ê³  ê°€ì •ì„ í•˜ì. embedding sizeëŠ” 16ì´ ë˜ëŠ”ë°, ê·¸ ì´ìœ ëŠ” userì™€ item vectorì— ëŒ€í•´ concatì„ í•´ì£¼ê¸° ë•Œë¬¸ì— embedding ê³¼ì •ì—ì„œ ì²« ë²ˆì§¸ hidden layer inputì˜ ì ˆë°˜ìœ¼ë¡œ embeddingí•´ì•¼ concatí–ˆì„ ë•Œ, sizeê°€ ê°™ì•„ì§„ë‹¤.

### 4.2 Performance comparison

![Untitled](/images/2022/NCF/t13.png)

NeuMFì˜ ì„±ëŠ¥ì´ ê°€ì¥ ì¢‹ìœ¼ë©°, ë‹¨ì¼ ëª¨ë¸ì˜ ê²½ìš°ì—ëŠ” ì˜¤íˆë ¤ MLPë³´ë‹¤ GMFì˜ ì„±ëŠ¥ì´ ë” ì¢‹ë‹¤.

### 4.3 Log loss with Negative sampling

![Untitled](/images/2022/NCF/t14.png)

Loss ê°’ì´ ì¤„ì–´ë“œëŠ” ê²ƒì„ í†µí•´ interaction functionì„ í•™ìŠµí•˜ëŠ”ë° ì ì ˆí•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.

### 4.4 Is deep learning helpful?

![Untitled](/images/2022/NCF/t15.png)

Layerì˜ depthê°€ ëŠ˜ì–´ë‚  ìˆ˜ë¡ ì„±ëŠ¥ì´ ê°œì„ ë˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤.
