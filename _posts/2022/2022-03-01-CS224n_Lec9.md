---
title: CS224n | Self-Attention and Transformers,Lec9
layout: post
Created: January 26, 2022 3:10 PM
tags:
    - Paper
    - Recommendation System
use_math: true
comments: true

---

>❗[Lecture Note](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/slides/cs224n-2021-lecture09-transformers.pdf)
>
>KIST 추천시스템을 만들어가는 과정 중, RNN→LSTM→Attention→Transformer 순으로 이어지는 메커니즘이 필요하여 시작되었습니다.

### Lecture 목차

1. RNN부터 attention based NLP model 복습.
2. Transformer model 소개
3. Great result with Transformers
4. Drawbacks and variants of Transformers

---

![Untitled](/images/2022/CS224n_9/t0.png)

- bidirectional LSTM을 사용하여 encode하고, LSTM을 이용하여 generate(decode)를 하였다. 이때 **sequence가 길어질 수록 bottleneck이 심해진다.** 그리고 Attention은 모든 state에 대해 기억되어진 encoded representation을 통해 decode되어진다.

![Untitled](/images/2022/CS224n_9/t1.png)
![Untitled](/images/2022/CS224n_9/t2.png)

- Recurrent Model을 사용하면 여러가지 문제가 존재한다.
    1. `Linear interaction distance`: O(sequence length)에 영향을 너무 많이 받는다. 즉, long-distance dependencies를 학습하는데 어려움이 있다.(gradient 문제때문에!)
    2. `Lack of parallelizability`: 기존의 RNN이나 LSTM은 time step에 종속적이기 때문에 최종적인 $T$를 계산하기 위해서는 그만큼의 state를 계산해야만 구할 수 있게 된다.

![Untitled](/images/2022/CS224n_9/t3.png)

- 1D convolution; word window model은 local context들을 결합(aggregate)하여, 이것을 사용하여 center에 대한 information을 나타낼 수 있다. 이 과정을 거쳐 layer를 더 거쳐간다.
- 독립적으로 word에 대해 embedding을 한다.(기존에는 올바른 embedding dimension out을 맞춰주기 위한 주변 word들에 대해 신경썼지만, 이젠 필요없다!)
- 핵심은 O(1) dependence in time이라는 것이다. 즉, sequence length가 증가하지 않는다는 것이다.

![Untitled](/images/2022/CS224n_9/t4.png)

- word window는 long-distance dependency를 알지 못한다. 하지만, word window를 stacking함으로써 더 멀리 잇는 word들에 대해서도 interaction을 고려할 수 있게 된다.
- Maximum Interaction distance **= sequence length / window size**
- 하지만, $h_k$를 학습할때, $h_1$에 대해서는 전혀 고려되지 않는다. 더 깊게 더 넒은 window size를 사용하면 알 수도 있지만, 실제에서는 finite field에 항상 있다.

![Untitled](/images/2022/CS224n_9/t5.png)

- Attention에서는 time에 대해서는 parallelizable하다. 하지만, layer에 대해서는 아직 parallelizable하지 않다.
    - RNN/LSTM에서 가지고 있던 문제 1)long-distance dependencies를 학습하는데 어려움이 있는 것 2)time에 대해 종속적이라는 것.

![Untitled](/images/2022/CS224n_9/t6.png)

- 편의상 Q(Query), Key(K), V(Valeu)라고 하자.
    - Self-Attention에서는 Q, K, V 동일한 d-dimension의 vector를 나타낸다.
    - $e_{ij}=q_i^\top k_j$ 식은 Q와 K의 affinity(연관성)을 dot-product를 통해 계산한다.
    - $\alpha_{ij}={exp(e_{ij})\over \sum_jexp(e_{ij'})}$ 식은 affinity로부터 softmax를 활용하여, attention weight들을 구한다.
    - $output_i= \sum_j \alpha_{ij}v_j$ 식은 weighted sum과 vector를 곱하여 다 더한 것이 output이 된다.

Q) fully-connected layer와 어떻게 다른가?

A) attention은 dynamic connectivity가 있고, inductive bias를 가진다. everything to everything으로 이어지는 feed forward가 아니다.

![Untitled](/images/2022/CS224n_9/t7.png)

- embedding으로부터 Q, K, V를 각각 얻어서 self-attention을 쌓아 연산을 한다.
- 하지만, self-attetnion은 고유한 order에 대한 개념이 없다.(RNN/LSTM model의 특성상 time-step이 있어서 문장을 순서대로 파악이 가능했지만, self-attention에는 그것을 order가 고려되지 않는다.)

![Untitled](/images/2022/CS224n_9/t8.png)

- self-attention은 order information이 없다. 따라서 Q, K, V의 문장 순서를 encode해줘야한다.
- $\tilde{v},\tilde{q},\tilde{k}$는 embedding한 것을 의미한다.
- $p_i$는 Q, K, V와 동일하게 d-dimension의 vector이며, sequence의 index를 나타낸다.(순서를 의미)
- self-attention block에 $p_i$를 단지 더해주기만 하면 된다!!

![Untitled](/images/2022/CS224n_9/t9.png)

- Sinusoidal position representation이 transformer paper나올 때 사용한 방법이다.
- sinusoids는 다양한 period를 가지고 있다. sinusoid를 시각화한 그림에서 vertical axis가 dimension을 의미하고, horizontal axis가 sequence length를 의미한다.
- Pros
    - sinusoid는 periodicity를 가진다. 따라서 서로 다른 index마다 다른 값으로 position을 표현할 수 있다.
    - longer sequence에 대해서도, periodr가 재시작하면 되기에 position을 표현할 수 있다.
- Cons
    - concat된 sinusoid에는 learnable parameter가 없고, extrapolation이 잘 작동하지 않는다.

![Untitled](/images/2022/CS224n_9/t10.png)

- 따라서 모든 $p_i$를 learnable parameter로 바꾼다.
- Pros
    - flexibility가 있다. 즉, 각 position마다 data에 적합한 학습을 하게 된다.
- Cons
    - 1부터 sequence length 사이에 있지 않고 초과되는 index를 extrapolate할 수 없다.

![Untitled](/images/2022/CS224n_9/t11.png)

- 고유한 order에 대한 개념없는 문제는 input에 position representation을 더해줌으로써 해결했다. 다음으로, 단순히 weigted average를 구하기 때문에 발생하는 nonlinearity하지 않다는 문제가 생긴다.

![Untitled](/images/2022/CS224n_9/t12.png)

- 이것도 마찬가지로 각 word들 마다 Feed-Foward Net을 더해줌으로써 nonlinearity를 만들어낼 수 있다.(Easy fix)

![Untitled](/images/2022/CS224n_9/t13.png)

- LSTM은 natural하다. 왜냐하면 time step이 존재하기 때문에! Language Modeling을 할 때, future word들을 반영하여 sequence를 예측하지 않는 것이 필요하다.

![Untitled](/images/2022/CS224n_9/t14.png)

- self-attention을 decoders에서 사용하기 위해 future에서 peek할 수 없다.
- 따라서 self-attention을 사용하기 위해 Masking을 사용한다.
- K, V set을 모든 time step 마다 past words만 포함시키면 변경할 수 있지만, 효율적이지 않다.
- Parallelization을 위해, attention을 mask한다. masking된 attention score들에 $-\infin$을 대입한다. $e_{ij}$을 구하는 수식처럼 position i이상의 position들에 대한 $e_{ij}$를 모두 masking하는 것이다.(future words를 반영하지 않게 하기 위해)

![Untitled](/images/2022/CS224n_9/t15.png)

- 마지막 3번째 문제도 해결.

![Untitled](/images/2022/CS224n_9/t16.png)

- self-attention building block을 만들때 필요한 요소들이다.
- Nonlinearity의 경우, 꼭 feed-forward network를 사용할 필요는 없다.(easy해서 설명한 듯하다.

![Untitled](/images/2022/CS224n_9/t17.png)

- high level에서 building block들을 보면, 다음과 같은 순으로 진행된다.

    word embedding + position representation → embedding → transformer encoder → (decoder에도 embedding + position representation가 input으로 들어간다.) transformer decoder (last layer of encoder가 모든 transformer decoder layer에 사용된다.) → predictions


![Untitled](/images/2022/CS224n_9/t18.png)

1. K, Q, V attention: single word embedding에서 K, Q, V vector들을 어떻게 얻을까?
2. Multi-headed attention: single layer에서 multiple place들에 attend 할거다.
3. 학습을 도와주는 tricks, 이것들은 traing process를 향상시키는데 돕는 것이지 model이 많은 것을 할 수 있게 함으로써 향상시키는 것이 아니다.
    1. Residual connections
    2. Layer normalization
    3. Scaling the dot product

![Untitled](/images/2022/CS224n_9/t19.png)

- transformer가 어떤 방식으로 same source로 부터 key-query-value를 가져오는지 알아본다.
    - $x_1, ..., x_T$ transformer encoder에 들어오는 Input vector들이다. dimensionality는 $d$이다.
- key, query, value
    - $k_i=Kx_i, K\in R^{d\times d}$이고, $K$는 key matrix이다.
    - $q_i=Qx_i, Q\in R^{d\times d}$이고,$Q$는 query matrix이다.
    - $v_i=Vx_i, V\in R^{d\times d}$이고, $V$는 value matrix이다.
- self-attention을 설명하는 과정에서 input vector x로부터 K, Q, V 모두 같다고 했지만, 이제는 각각 대응하는 matrix와 linear transformation을 하기 때문에 조금씩 다르다.
- 3개의 역할이 각각이 다르다. K, Q는 어딜 봐야하는지 도와주며, V는 information에 접근하게 도와준다.

![Untitled](/images/2022/CS224n_9/t20.png)

- 어떻게 key-query-value attention이 계산되어지는지 알아보자.
    - $X=[x_1, ..., x_T] \in \mathbb{R^{T\times d}}$의 input vector들을 concat한다.
    - input vector와 dot product를 통해 $XK\in\mathbb{R^{Td}},XQ\in\mathbb{R^{Td}},XV\in\mathbb{R^{Td}}$를 만든다.
    - query와 key의 dot product를 통해 하나의 matrix를 만든다.
        - $XQ(XK)^T$
    - softmax를 활용하여 weighted average를 계산한다.
        - $output=softmax(XQ(XK)^T)\times XV$

![Untitled](/images/2022/CS224n_9/t21.png)

- 어떻게 sentence의 multiple place를 한 번에 attend할까? normal self-attention으로도 가능하지만, $x_i^TQ^TKx_i$가 높은 값에 대한 place를 보게 된다. 다른 이유들로 다른 place $j$를 focus하고 싶으면 어떻게 해야하나?
- 따라서 multi attention heads라는 것을 multiple Q, K, V를 통해 정의한다.
- Q, K, V를 대신하여 sub Q, sub K, sub V($Q_l, K_l,V_l\in\mathbb{R}^{d\times {d\over h}}$)를 사용한다. $h$는 attention head의 개수, $l$은 1부터 $h$사이의 값을 의미한다.
- softmax를 사용하여 Output을 만들어 주게된다.
- 모든 head(h)의 output을 concat시켜서 total output을 만들어 낸다. 즉, output dimensionality와 input dimensionality를 동일하게 만들기 위함이다.
- 각 attention head는 독립적으로 동작한다.

![Untitled](/images/2022/CS224n_9/t22.png)

- Multi-head attention이라도 single-head보다 더 많은 계산을 하는 것은 아니다. 단지 기존의 Q를 나눠서 연산할 뿐이다!

![Untitled](/images/2022/CS224n_9/t23.png)

- model이 학습을 더 잘하도록 도와주는 trick 1번! Residual connections
- $i$는 layer의 depth를 나타낸다.
- residual connection은 simple하다.
    - $X^{(i)} = X^{(i-1)} + Layer(X^{(i-1)})$
    - 이전 layer로부터의 residual(잔차)를 학습하게끔한다.
    - $X^{(i-1)}$은 inductive bias역할을 한다.
- gradient vanishing 문제를 해결한다. 즉, layer를 통해 vanishing된다고 하더라도 residual connection, $X^{(i-1)}$이 propagate될 수 있다.
- 학습이 좀 더 smooth해진다.

![Untitled](/images/2022/CS224n_9/t24.png)

- model이 학습을 더 잘하도록 도와주는 trick 2번! Layer normalization
- 학습에 피해를 주는 a lot of uninformative들을 각각의 layer마다의 mean과 std를 통해 normalizing해주는 것이다. 이를 통해 cut down을 하고자 한다.
- $\epsilon$은 $\sigma$가 아주 작거나 NaN일 경우를 대비해 더해주는 역할을 한다.

![Untitled](/images/2022/CS224n_9/t25.png)

- model이 학습을 더 잘하도록 도와주는 trick 3번! Scaled Dot Product
- dot product를 scaling하는 것이다. transformer의 dimensionality가 아주 크다. 이때문에 vector간의 dot product 또한 아주 클 수 밖에 없다. 이것이 바로 softmax에 들어가게 되면 peaky한 shape을 가지게 되고, gradient 값들이 아주 작아지게 된다. softmax는 hidden vector의 weight를 만드는 역할을 하는데, 대부분의 weight들이 너무 작아져 0이 되면, attend하는 효과가 나타나지 않게 된다. 따라서 $\sqrt{d/h}$로 scaling하는 것이다.
- Q ) decoder attention에서, first layer에 대해서만 masking하는 것인가 아니면 middle layer에 대해서도 masking하는 것인가.
- A) 당연하게도, decoder의 모든 single layer가 masking을 한다. encoder의 모든 state를 볼 수 있고, decoder에서는 masking으로 인해 previous words에 대해서만 볼 수 있다.

![Untitled](/images/2022/CS224n_9/t26.png)

- encoder의 각 layer들은 해당 block들과 동일하게 이루어져있다.

![Untitled](/images/2022/CS224n_9/t27.png)

- decoder는 encoder와 다르게 masking이 추가되어 있다.
- Multi-Head Cross-Attention은 기존의 attention과 유사하다.(decoder to encoder라서)

![Untitled](/images/2022/CS224n_9/t28.png)

- Multi-head cross attention은 mutli-head self attention과 동일한 equation이지만, input이 다른 곳(encoder의 last output)으로 부터 들어오기 때문에 자세히 다뤄보려 한다.

![Untitled](/images/2022/CS224n_9/t29.png)

- self attention에서는 keys, queries, values가 모두 같은 source로부터 들어왔다. decoder의 Cross-attention에는 transformer encoder의 마지막 output vector $h_1,...,h_T$를 $k_i=Kh_i, v_i=Vh_i$로 사용되며, decoder의 input은 query를 계산하는 $q_i=Qz_i$로 사용된다.
- 해당 방식은 memory에 access하는 것과 유사하다.

![Untitled](/images/2022/CS224n_9/t30.png)

- query-key dot product를 하고, softmax를 사용해서 계산된 weighted average를 사용한다.

![Untitled](/images/2022/CS224n_9/t31.png)

- Machine Translation에 있는 table로, Attention is all You need paper에 있다.

![Untitled](/images/2022/CS224n_9/t32.png)

- perplexity는 낮아질 수 록 좋은것이다. ROUGE-L에 대해서는 높을 수 록 좋은것이다.

![Untitled](/images/2022/CS224n_9/t33.png)

- 실제로, transformer의 parallelizablity 때문에 dominant가 되었다.

![Untitled](/images/2022/CS224n_9/t34.png)

- transformer의 pain point는 self-attention에서 quadratic compute하다. RNN 계열의 model은 linear하게 grow한다.
- position을 표현하는데 있어서 simple하지 않다.

![Untitled](/images/2022/CS224n_9/t35.png)

- Practice에서는 sequence length $T$는 보통 512정도의 값을 갖는다. 하지만 만약 $T$≥10000의 long document에 대해 학습할까. 어떠한 방법으로든 $T^2$를 최적화해야한다.

![Untitled](/images/2022/CS224n_9/t36.png)

- 위의 문제점에 대한 한가지 해결책은 Linformer이다. key idea는 sequence length dimension을 lower dimension space로 mapping 시키는 것이다.

![Untitled](/images/2022/CS224n_9/t37.png)

- 완전히 다른 접근방법으로, all-pairs interaction을 고려하지 말자는 것이다. all-pairs interaction을 하는 것보다 다른 더 효율적인 방법으로 이를 골려해보자는 것이다. 예를들어 local window, looking at everything, random interaction.
- BigBird는 이런 것들을 모두 합쳐서, all-pair interaction과 유사한 결과를 얻을 수 있다고 한다.
