---
title: CS231n | Recurrent Neural Networks,Lec10
layout: post
Created: January 20, 2022 2:58 PM
tags:
    - CS231n
use_math: true
comments: true

---

>❗ [Lecture Note](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf)

![Untitled](/images/2022/CS231n_10/t0.png)

Image 객체를 보고, Image를 설명하는 sequence of words로 출력할 수 있다.

![Untitled](/images/2022/CS231n_10/t1.png)

sequence of words를 보고 긍/부정을 출력할 수도 있고, video의 frame을 보고, 어떤 영상인지 어떤 활동이 일어나는지 classification을 할 수 있다.

![Untitled](/images/2022/CS231n_10/t2.png)

input의 영어문장과 output의 프랑스어 문장의 길이가 다양할 수 있다.

![Untitled](/images/2022/CS231n_10/t3.png)

![Untitled](/images/2022/CS231n_10/t4.png)

time step마다 어떠한 출력을 만들어 낸다고 한다면, 각 time step마다 input인 $x_t$와 이전 hidden state의 $h_{t-1}$를 읽어드린 다음 Fully Connected Layer를 붙여서 Ouput을 만들어 낸다.

![Untitled](/images/2022/CS231n_10/t5.png)

주의할 점은 time step마다 사용되는 계산은 같은 function을 쓰고 초기에 설정한 같은 Weight를 사용한다는 것이다.

![Untitled](/images/2022/CS231n_10/t6.png)

보통의 경우에 $h_0$은 zero-state로 initialization하며, W(weight)를 공유하며 학습한다. 각 cell마다 ground truth label이 있다면, real label과의 loss를 구하여 총합을 계산해 softmax loss를 활용할 수 있다.

![Untitled](/images/2022/CS231n_10/t7.png)

sentiment analysis(감성분석)처럼 마지막의 hidden state가 결국 모든 sequence를 summarize한다고 할 수 있다.

![Untitled](/images/2022/CS231n_10/t8.png)

고정된 input에 대해 많은 output을 구성할 수도 있다.

![Untitled](/images/2022/CS231n_10/t9.png)

Sequence to Sequence의 예시로 Machine Translation이 있다. 이를 소위 Many to one 과정을 encoder, one to many를 decoder의 과정으로 나눌 수 있다. encoder에서는 sequence를 요약하고, 요약된 vector를 decoder에 input값으로 넣음으로써 time step마다 어떤 예측을 한다. 이때 나온 loss들을 다 더해서 backpropagation을 수행할 수 있다.

![Untitled](/images/2022/CS231n_10/t10.png)

time step에 대해 한꺼번에 forward pass를 하고 난뒤, loss를 구해서 backward pass를 하는 것은 gradient descent 할 때마다 매우 느리다. 또, 절대 수렴하지 않게 될 것이며, 상당한 메모리양을 필요로 한다.

![Untitled](/images/2022/CS231n_10/t11.png)

![Untitled](/images/2022/CS231n_10/t12.png)

![Untitled](/images/2022/CS231n_10/t13.png)

위에 대한 해결책으로 truncated backpropagation을 할 수 있다. 지정해 놓은 time step만큼씩 forward와 backward를 계산해 나가는 것이다. 이는 SGD와 흡사하며, 전체 dataset을 한 번에 학습시키는 것이 어렵고 효율적이지 않기 때문에 작은 mini batch를 취한다고 할 수 있다.

![Untitled](/images/2022/CS231n_10/t14.png)

Image 하나를 보고 CNN을 거쳐서 하나의 vector로 표현한 것을 첫 번째 time step의 input으로 넣는다. 이를 통해 RNN을 거치게 되면 Image에 대한 설명을 sequence로 나타낼 수 있게 된다.

![Untitled](/images/2022/CS231n_10/t15.png)

CNN의 마지막 layer에서 softmax를 하는 것이 아니라, 그 전의 Fully Connected Layer의 vector를 initial hidden state에 추가하여 다음 hidden state를 계산한다. 이때, 이미지 vector는 모든 time step마다 더해진다. backpropagation은 CNN의 마지막 layer에 전달되고, 추가적으로 CNN의 weiht들도 업데이트해서 model 전반적인 parameter들을 튜닝해서 작동되게 된다.

![Untitled](/images/2022/CS231n_10/t16.png)

CNN이 전체 이미지를 요약하는 단일 vector를 생성하는 것이 아니라, vector의 grid를 만들어 이미지의 각 공간 위치 마다 하나의 vector를 제공한다. initial hidden state에서는 이미지에 대한 분포를 계산하고 다시 gird로 돌아가서 z1의 요약된 vector를 제공하고 그것은 h1의 입력으로 들어가게 된다. 이때 h1의 output은 이미지에 대한 분포와 어휘 단어들에 대한 분포 두 가지를 출력한다.

![Untitled](/images/2022/CS231n_10/t17.png)

soft attention은 모든 이미지의 위치로부터 모든 feature들의 조합을 얻는다.

hard attention은 모델의 time step마다 이미지에서 정확한 지점들만 선택하도록 강요하는 것이다.

![Untitled](/images/2022/CS231n_10/t18.png)

RNN에서는 보통 2,3,4개의 layer만 쌓아도 충분히 깊다.

![Untitled](/images/2022/CS231n_10/t19.png)

![Untitled](/images/2022/CS231n_10/t20.png)

![Untitled](/images/2022/CS231n_10/t21.png)

$h_0$에 대한 손실의 gradient를 계산할 때 지나쳐온 cell들을 다 통과하면서 backpropagation해야한다. 예시와 다르게 cell의 개수가 100개처럼 많다고 했을 때, weight값이 1보다 크다면 exploding gradient 문제가 발생하고, weight값이 1보다 작다면 vanishing gradient 문제가 발생한다. 전자의 경우에는 gradient clipping 즉, 정규화한 gradient가 너무 크면 gradient를 조정할 수 있다(L2 norm을 활용). 후자의 경우에는 RNN의 architecture를 더 복잡하게 변경해야한다.

![Untitled](/images/2022/CS231n_10/t22.png)

![Untitled](/images/2022/CS231n_10/t23.png)

Weight와 4개의 다른 게이트들을 계산하여 활용한다.

- input gate는 sigmoid(0~1)를 써서 얼마나 많이 cell에 입력하고 싶은지
- forget gate는 sigmoid(0~1) 이전 time step으로부터 얼마나 cell memory를 잊을 건지
- output gate는 sigmoid(0~1) 얼마나 외부로 노출시킬건지
- gate gate는 tanh(-1~1) 얼마나 많이 우리의 input cell에다가 쓰고 싶은지

$$
c_t = f \odot c_{t-1} + i\odot g
$$

$f$로 특정 cell의 원소들을 잊을지 말지를 결정한다.(1이면 기억하고, 0이면 기억하지 않는다.)

LSTM의 GD과정은 RNN보다 비교적으로 interupt없이 첫번째 time step까지 쭉 지나가며 계산할 수 있다.

![Untitled](/images/2022/CS231n_10/t24.png)

LSTM의 uninterrupted gradient flow는 ResNet과 유사해 보인다.

![Untitled](/images/2022/CS231n_10/t25.png)

GRU와 LSTM에서 진화를 거듭해봤지만, 이 2개가 보편적으로 좋다. 2가지는 GD를 +, $\odot$을 적절히 활용하여 적절히 관리하는 아이디어가 유용하다.

![Untitled](/images/2022/CS231n_10/t26.png)

---
#### Reference
[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
[LSTM(Long Short Term Memory networks)](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=apr407&logNo=221237917815)
[RNN과 LSTM을 이해해보자!](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)
[CS 230 - Recurrent Neural Networks Cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)
