---
title: CS231n | Introduction to Neural Networks,Lec4
layout: post
Created: January 19, 2022 7:18 PM
tags:
    - CS231n
use_math: true
comments: true

---

>❗ [Lecture Note](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture4.pdf)


![Untitled](/images/2022/CS231n_4/t0.png)

Backpropagation(역전파)의 keypoint는 chain rule을 연속적으로 사용하여, 모든 변수들에 대한 gradient를 계산할 수 있다.

![Untitled](/images/2022/CS231n_4/t1.png)

add gate는 gradient를 각각의 gate에 동일하게 부여한다. max gate는 gradient의 router라고 할 수 있다. mul gate는 local gradient값이 다른 변수에 대한 값을 의미한다. 따라서 gradient switcher라고 할 수 있다.

![Untitled](/images/2022/CS231n_4/t2.png)

forward를 할 때에는 forward 방향으로 연결되어 있는 모든 node에 영향을 준다. 따라서 모든 노드들은 backward를 할 때에도 root node에 영향을 주며, upstream gradient의 총합이 계산되어 진다.

![Untitled](/images/2022/CS231n_4/t3.png)

Scalar가 아니라 Vector인 경우에는 위의 예시들과 다르게, gradient들이 Jacobian Matrix가 된다는 것이다.

![Untitled](/images/2022/CS231n_4/t4.png)

<div class="center">
  <figure>
    <a href="/images/2022/CS231n_4/c0.jpeg"><img src="/images/2022/CS231n_4/c0.jpeg" width="500"  ></a>
  </figure>
</div>

유의해야할 점은 Matrix의 element별로 gradient가 나와야한다. 즉, 기존의 matrix와 gradient의 shape이 같아야한다.

![Untitled](/images/2022/CS231n_4/t5.png)

<div class="center">
  <figure>
    <a href="/images/2022/CS231n_4/c1.jpeg"><img src="/images/2022/CS231n_4/c1.jpeg" width="500"  ></a>
  </figure>
</div>
마찬가지로 matrix의 shape을 맞춰준다.

![Untitled](/images/2022/CS231n_4/t6.png)

- Neural Net이 커짐에 따라 모든 parameter에 대해 gradient를 계산하는 것은 사실상 불가능하다.
- Backpropagation은 Chain Rule이 반복적으로 적용되어 최종적으로 모든 입력에 대한 gradient를 계산하게 된다.
- Forward
는 연산의 결과를 계산하고 gradient 계산에 필요한 중간값들을 메몸리에 저장한다.
- Backward는 chain rule을 적용하여 입력에 대한 loss function의 gradient를 계산한다.

![Untitled](/images/2022/CS231n_4/t7.png)

교안에 나온 보기 처럼 Linear Layer를 거치고 Non Linear Layer를 거치는 것이 매우 중요하다. 왜냐하면, linear layer만으로 hidden layer을 구성하면, 하나의 linear layer가 있는 것과 동일하기 때문이다.

따라서 NN은 단지 Complex Non-Linear Function이라고 말할 수 있다.

> 신경망에서 비선형 함수를 사용하지 않으면, hidden layer가 없는 network와 똑같은 기능을 하게 된다.
> -밑바닥부터 시작하는 딥러닝1, chap 3.2-

![Untitled](/images/2022/CS231n_4/t8.png)

Non-Linear을 적용하기 위한 Activation Function들이다.

![Untitled](/images/2022/CS231n_4/t9.png)

해당 예시들은 input layer와 output layer을 제외하고, hidden layer을 쌓은 모습이다. architecture를 보아 Fully-Connected NN인 것을 알 수 있다.

![Untitled](/images/2022/CS231n_4/t10.png)
