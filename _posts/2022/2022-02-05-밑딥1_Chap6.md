---
title: 밑바닥부터 시작하는 딥러닝1 | 학습 관련 기술들,Chap6
layout: post
Created: February 5, 2022 4:55 PM
tags:
    - 밑딥1
use_math: true
comments: true

---

# Parameter Update(Optimization Method)

- NN 학습의 목적은 loss function의 값을 최대한 낮추는 parameter를 찾는 것이다. 이는 곧 optimal parameter를 찾는 문제로 접근하게 되며, 이를 곧 **optimization**이라고 한다.

## SGD(Stochastic Gradient Descent)

$$W \leftarrow W-\eta {\partial L\over \partial W} $$

- 해당 식은 **SGD**의 수식이다. 여기서 $\eta$는 learning rate를 의미한다. 다음은 SGD를 python 구현한 것이다.

```python
class SGD:
	def __init__(self, lr=0.01):
		self.lr = lr

	def update(self, params, grads):
		for key in params.keys():
			params[keys] -= self.lr * grads[key]
```

![Untitled](/images/2022/밑딥1_6/t0.png)

- **SGD의 단점** : anisotropy function(비등방성함수, 방향에 따라 gradient가 달라지는 함수)에서는 optimal point를 찾는 경로가 비효율적이라는 것이다. 따라서 무작정 learning rate에 따라 기울어진 방향으로 진행하는 단순한 방식은 학습에 효율적이지 않게 되는 것이다.

    → Momentum, AdaGrad, Adam이 SGD를 개선하고 대체한 optimization 기법들이다.


## Momentum

$$
v \leftarrow \alpha v-\eta {\partial L\over \partial W}
$$

$$
W \leftarrow W-v
$$

- 해당 식은 Momentum의 수식이다. SGD와 동일하게 $\eta$는 learning rate를 의미한다. $v$는 물리에서 말하는 velocity(속도)를 의미한다. 다음은 **Momentum**을 python으로 구현한 것이다.

```python
class Momentum:
    def __init__(self, lr=0.01, momentum=0.9):
        self.lr = lr
        self.momentum = momentum
        self.v = None

    def update(self, params, grads):
        if self.v is None:
            self.v = {}
            for key, val in params.items():
                self.v[key] = np.zeros_like(val)

        for key in params.keys():
            self.v[key] = self.momentum * self.v[key] - self.lr * grads[key]
            params[key] += self.v[key]
```

![Untitled](/images/2022/밑딥1_6/t1.png)

- SGD보다 지그재그한 정도가 덜하다.

## AdaGrad

- learning rate(lr, 학습률) 값은 NN을 학습하는 과정에서 중요하다. 너무 작으면 학습 시간이 길어지게 되고, 반대로 너무 크면 발산하여 올바른 학습이 되지 않는다. 따라서 학습률을 정하는 효과적인 기술로 learning rate decay(학습률 감소)가 있다. 학습을 진행하는데 있어서 학습률을 점차 줄여가는 방법으로 학습 초기에는 큰 lr로 학습하다가 후반으로 갈 수록 lr을 감소시켜 학습한다.
- lr을 서서히 낮추는 가장 간단한 방법은 parameter 전체의 lr을 일괄적으로 낮추는 것이다. **AdaGrad**는 각각의 parameter에 맞춤형 값(lr)을 만들어 낸다.

$$
h \leftarrow h+{\partial L\over \partial W}\odot {\partial L\over \partial W}
$$

$$
W \leftarrow W +\eta {1\over \sqrt{h}} {\partial L\over \partial W}
$$

- $\odot$은 element-wise product(matrix의 element별 곱셈)를 의미한다. $h$는 기존의 gradient값을 제곱하여 계속 더해준다. 그리고 parameter를 update할 때, ${1\over \sqrt{h}}$를 곱해 lr을 조정한다. 즉, parameter의 element들 중에 크게 update된 element는 lr이 낮아진다는 것을 의미한다.
- AdaGrad는 과거의 gradient를 제곱하여 계속해서 더하기 때문에 학습을 진행함에 따라 update가 더딜 수 밖에 없다. 실제로 무한히 학습한다고 했을 때, update양이 0이 되어 전형 update하지 않게 된다. 이 문제를 개선한 optimization이 **RMSProp**이다. RMSProp는 과거의 모든 gradient를 균일하게 계속 더하는 것이 아니라, 먼 과거의 gradient는 서서히 잊고 최근의 gradient의 정보를 크게 반영한다. 이를 EMA(Exponential Moving Average, 지수이동평균)이라고 하며, 과거의 gradient의 반영하는 규모를 기하급수적으로 감소시킨다.
- 다음은 AdaGrad을 python으로 구현한 것이다.

```python
class Momentum:
    def __init__(self, lr=0.01):
        self.lr = lr
        self.h = None

    def update(self, params, grads):
        if self.h is None:
            self.h = {}
            for key, val in params.items():
                self.h[key] = np.zeros_like(val)

        for key in params.keys():
            self.h[key] += grads[key] * grads[key]
            params[key] -= (self.lr / (np.sqrt(self.h[key]) + 1e-7)) * grads[key]
```

![Untitled](/images/2022/밑딥1_6/t2.png)

- 1e-7을 더해주는 이유는 `self.h[key]`에 0이 있다고 하더라도 0으로 나눠지는 사태를 막아준다. 이 값도 hyperparameter로 설정 가능하다.

## Adam

![Untitled](/images/2022/밑딥1_6/t3.png)

- Momentum은 공이 굴러가듯 velocity를 update에 활용하였고, AdaGrad는 parameter의 element마다 학습이 진행됨에 따라 lr을 조정하는 방안을 활용했습니다. 이 두 가지 방법(Momentum + AdaGrad에서 발전한 RMSProp)을 융합한 것이 **Adam**이다.
- Adam은 hyperparameter의 bias correction(편향 보정)이 진행된다.
- Adam은 hyperparameter가 총 3개로, lr(논문에서는 $\alpha$), 1차 Momentum 계수 $\beta_1$, 2차 Momentum 계수 $\beta_2$가 있다. default는 $\beta_1$(0.9) $\beta_2$(0.999)이다.

# Weight Initialization

- weight initialization은 학습에 있어서 중요한 요소이다.
- weight initialization을 모두 0으로 하는 것 뿐만 아니라 initial value가 같은 값이더라도 학습이 제대로 이루어 지지 않는다. 왜냐하면, backpropagation에서 weight 값들이 동일하게 update되기 때문이다.

```python
import numpy as np
import matplotlib.pyplot as plt

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def ReLU(x):
    return np.maximum(0, x)

def tanh(x):
    return np.tanh(x)

x = np.random.rand(1000, 100) # feature의 개수가 100개인, 1000개의 데이터
node_num = 100 # 각 hidden layer의 node의 수
hidden_layer_size = 5 # hidden layer 개수
activations = {} # 해당 dictionary에 activation 결과를 저장

for i in range(hidden_layer_size):
    if i != 0:
        x = activations[i-1]

    w = np.random.randn(node_num, node_num) * 1
    # w = np.random.randn(node_num, node_num) * 0.01
    # w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)
    # w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)

    a = np.dot(x, w)

    z = sigmoid(a)
    # z = ReLU(a)
    # z = tanh(a)

    activations[i] = z

for i, a in activations.items():
    plt.subplot(1, len(activations), i+1)
    plt.title(str(i+1) + "-layer")
    if i != 0:
        plt.yticks([], [])
		# plt.xlim(0.1, 1)
    # plt.ylim(0, 7000)
    plt.hist(a.flatten(), 30, range=(0,1))
plt.show()
```

![Untitled](/images/2022/밑딥1_6/t4.png)

- sigmoid function이며 weight의 표준편차를 1로 설정한 경우, gradient vanishing 문제가 발생한다.

![Untitled](/images/2022/밑딥1_6/t5.png)

- weight의 표준편차를 0.01로 바꿔 실험한 경우, gradient vanishing이 일어나지는 않았다. 하지만, 특정 값에 activation 값들이 몰려 있다는 말은 즉, 다수의 node들이 같은 값들을 출력하고 있기에 node를 여러개 설정한 의미가 없어지게 된다.
- 이는 node가 1개의 성능과 100개의 성능의 차이가 존재하지 않게 된다.

![Untitled](/images/2022/밑딥1_6/t6.png)

- Xavier initialization 논문에서는 1) $\sqrt{2\over n+m}$를 제안했는데 caffe와 같은 framework에서는 2) $\sqrt {1\over n}$로 단순화 시켰다.
- 해당 경우는 2)방법을 사용했다.
- layer가 깊어질 수록 정규분포의 모양에서 벗어나고 있는데, sigmoid를 tanh으로 변경하면 개선된다.
- 지금까지는 sigmoid activation func을 random init부터 Xavier init까지 다뤄봤다. 다음으로는 ReLU activation func에 대한 initialization을 다뤄보고자 한다.
- sigmoid나 tanh는 y축 대칭이라 Xavier가 적당하다. ReLU는 음의 영역이 0값으로 대체 되기때문에 더 넓게 분포시키기 위해 2배의 계수가 필요하다고 해석할 수 있다.

![Untitled](/images/2022/밑딥1_6/t7.png)

std 0.01에 대한 random init

![Untitled](/images/2022/밑딥1_6/t8.png)

Xavier init

![Untitled](/images/2022/밑딥1_6/t9.png)

He init

- ReLU에서는 He initalization이 gradient vanishing 문제에 대해 좀 더 적극적으로 대응할 수 있는 방법이다.

# [Batch Normalization](https://www.notion.so/Batch-Normalization-6238c0d6ef52476c91f2f49e40dd1967)

- 2015년에 제안된 방법으로 몇 가지 문제들에 대한 개선을 야기한다.
    - **학습 속도 개선**
    - **initialization에 크게 의존하지 않는다.**
    - **overfitting을 억제한다. → dropout의 필요성이 감소되는 효과**

# Overfitting

- overfitting이 주로 일어나는 경우
    - parameter의 개수가 많고, 데이테에 대한 표현력이 높은 모델
    - training dataset이 부족한 경우

# Weight Decay

- overfit을 조절하는 방법으로 **weight decay**(가중치 감소)가 있다. 이는 학습하는 과정에서 큰 weight들에 대해 그에 상응하는 큰 penalty를 부여하는 것이다.
- 예를 들어, weight가 $W$라고 하자. L2 법칙에 따라 weight decay는 ${1\over2}\lambda W^2$가 된다. 그리고 ${1\over2}\lambda W^2$를 loss function에 더한다. 이때, $\lambda$는 normalization의 세기를 조절하는 hyperparameter를 의미한다. 즉, $\lambda$를 크게 설정할 수 록 큰 weight에 대해 penalty가 커지게 된다.

# Dropout

- weight decay만으로 더 복잡한 NN에 대해 대응하기 어렵다. 이 때 사용되는 기법이 **dropout**이다.
- dropout은 무작위로 node를 삭제하여 training한다. 단, test를 할 때는 각 node의 output에 train에 삭제한 비율을 곱하여 출력한다.

```python
import os
import sys
sys.path.append(os.pardir)  # 부모 디렉터리의 파일을 가져올 수 있도록 설정
import numpy as np
import matplotlib.pyplot as plt
from dataset.mnist import load_mnist
from common.multi_layer_net_extend import MultiLayerNetExtend
from common.trainer import Trainer

(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)

# 오버피팅을 재현하기 위해 학습 데이터 수를 줄임
x_train = x_train[:300]
t_train = t_train[:300]

# 드롭아웃 사용 유무와 비울 설정 ========================
use_dropout = True  # 드롭아웃을 쓰지 않을 때는 False
dropout_ratio = 0.2
# ====================================================

network = MultiLayerNetExtend(input_size=784,
                              hidden_size_list=[100, 100, 100, 100, 100, 100],
                              output_size=10, use_dropout=use_dropout,
                              dropout_ration=dropout_ratio)
trainer = Trainer(network, x_train, t_train, x_test, t_test,
                  epochs=301, mini_batch_size=100,
                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)
trainer.train()

train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list

# 그래프 그리기==========
markers = {'train': 'o', 'test': 's'}
x = np.arange(len(train_acc_list))
plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)
plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)
plt.xlabel("epochs")
plt.ylabel("accuracy")
plt.ylim(0, 1.0)
plt.legend(loc='lower right')
plt.show()
# epoch:301, train acc:0.73, test acc:0.6315
```

- forward할 때 신호를 통과시키는 node는 backward할 때도 그대로 통과시키고, forwar때 통과시키지 않은 node는 마찬가지로 backward할 때도 신호를 차단합니다.

# Select Hyperparameter

- hyperparameter를 tuning할 때는 validation dataset으로 최적화해야한다.
- train data는 parameter를 학습하는 용도! validation data는 hyperparameter의 성능을 평가하는 용도! test data는 model 자체의 performance를 평가하는 용도!
- grid search보다는 random sampling search가 보편적으로 좋은 결과를 나타낸다.
- **hyperparameter optimization step**
    - 0 step: hyperparameter의 범위 설정
    - 1 step: 설정된 범위에서 무작위로 hyperparameter 추출
    - 2 step: 1 step에서 sampling한 hyperparameter 값을 사용하여 training하고, validation으로 평가한다.(단, epoch은 작게 설정한다.)
    - 3 step: 1,2 step을 특정 횟수(100회 등)를 반복하여, performance를 보고 hyperparameter의 범위를 좁힌다.
