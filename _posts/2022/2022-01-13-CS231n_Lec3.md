---
title: CS231n | Loss Functions and Optimization,Lec3
layout: post
Created: January 13, 2022 10:22 PM
tags:
    - CS231n
use_math: true
comments: true

---


>❗ [Lecture Note](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf)


![Untitled](/images/2022/CS231n_3/t0.png)

$L$에 대한 설명 :  $x_i$는 sample, $W$는 weight이다. 이 두가지를 통해 $f$에 input하여 예측한 값과 실제 $y_i$와의 loss를 계산한다. 데이터 각각에 따른 loss값을 다 더하고 데이터의 개수로 나눈다. 이것을 Loss function이라고 한다.

![Untitled](/images/2022/CS231n_3/t1.png)

실제 image label의 loss가 $s_{y_i}$가 가장 높게 나타나는 다른 image label의 loss $s_j$와 동일하더라도 loss를 1로 판단한다. 이는 동일한 W를 가지고 있다면, 정답인 label에 더 학습을 시키기 위함이다.

1번 sample의 loss를 구하면, max(0, 5.1-3.2+1) + max(0, -1.7-3.2+1) = 2.9 + 0 = 2.9

2번 sample의 loss를 구하면, max(0, 1.3-4.9+1) + max(0, 2.0-4.9+1) = 0 + 0 = 0

3번 sample의 loss를 구하면, max(0, 2.2-(-3.1)+1) + max(0, 2.5-(-3.1)+1) = 6.3 + 6.6 = 12.9

$L$(Loss) = (2.9 + 0 + 12.9) / 3 = 5.27

- Q1) What happens to loss if car scores change a bit?

Ans) 변함없이 loss는 0이다. 이미 다른 w들보다 충분히 크기 때문이다.

- Q2) What is the min/max possible loss

Ans) min : 0, max : ∞(infinity)

- Q3) At initialization W is small so all s ≈ 0. What is the loss

Ans) 클래스 수 - 1

- Q4) What if the sum was over all classes(including j = y_i)

Ans) loss increases by one

- Q5 What if we used mean instead of sum

Ans) it doesn’t change 변하지 않는다!

![Untitled](/images/2022/CS231n_3/t2.png)

loss가 우연히 0이라고 할 때, 특정 weight가 유일한 것은 아니다

![Untitled](/images/2022/CS231n_3/t3.png)

Data Loss는 모델의 예측이 학습 데이터(파란색 데이터)와 match되어야 한다. 하지만 테스트 데이터(초록색 데이터)는 이런 모양을 맞춰야한다고 하자. 이 과정에서 R의 역할은 모델이 simple하게 동작하는 것을 돕는다. hyperparameter($\lambda$)는 loss 와 R사이의 trade off 관계에 있기에 tuning해줘야 한다. 예를 들어보자 dropout을 많이 하면, 그만큼 학습데이터에 overfit될 것이고, 적게하면 그만큼 simple해져서 덜 overfit될 것이다. 그를 생각하면 된다.

![Untitled](/images/2022/CS231n_3/t4.png)

sort of penalize somehow the complexity of the model rather than explicitly trying to fit the training data

→ model의 복잡성을 줄여준다!

L2 정규화는 미분을 없애줘서 좋다.

L1 정규화의 특징으로 예를 들어 W의 sparsity를 증가시킨다.

![Untitled](/images/2022/CS231n_3/t5.png)

multiclass SVM Loss function에서 loss 값들은 그 값 자체가 중요한 것이 아니라 예측한 값들 중 가장 큰 값과 실제 정답인 class의 값, 두개의 차이가 중요한 것이었다. 즉, 정답인 class의 값이 틀린 class의 값보다 커야한다는 말이다. 하지만 Softmax에서는 이 값들에 의미를 부여한다.

![Untitled](/images/2022/CS231n_3/t6.png)

모든값을 지수화 하여 양수로 만든 다음, 그것들의 합으로 다시 정규화하면, 각 class의 확률 값이 나온다.

이때 우리는 해당 class의 확률을 1에 가깝게 해야하는데, 수학적으로 raw 확률을 최대화하는 방법보다 log를 최대화 하는 것이 더 쉽다. 그래서 -log를 사용하여 최대화 한다.

확률값이라 0과 1사이의 값이 -log에 들어가기 때문에 min loss는 0이고, max loss는 infinity이다.

![Untitled](/images/2022/CS231n_3/t7.png)

Softmax와 SVM의 큰 차이는 없지만, Softmax는 끊임없이 틀린 class는 0에 가깝게 맞는 class는 1에 가깝게 계속해서 반복한다. 하지만, SVM loss는 틀린 class의 값과 맞는 class의 값의 비교를 통해 조건이 충족한다면, 거기에서 끝.

![Untitled](/images/2022/CS231n_3/t8.png)

그렇다면, 우리가 학습데이터를 가지고 정답지와의 loss를 줄여나가는 것은 알겠다. 그렇다면, loss를 줄여나가면서 최적의 W를 찾는 과정이 바로 optimization이다.

![Untitled](/images/2022/CS231n_3/t9.png)

W를 업데이트하는데 모든 데이터를 한번에 하려면 학습하는데 시간이 너무 오래걸린다. 따라서 SGD를 사용한다. mini batch만큼의 sampling을 하고, 그것을 통해 W를 업데이트한다.
