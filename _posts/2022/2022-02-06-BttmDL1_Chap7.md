---
title: 밑바닥부터 시작하는 딥러닝1 | CNN,Chap7
layout: post
Created: February 6, 2022 4:26 PM
tags:
    - DL
    - 밑딥1
use_math: true
comments: true

---

# CNN Architecture

- Convolutional Layer, Pooling Layer가 필요하다.

## Convolutional Layer

- Fully-Connected Layer의 문제점은 데이터의 형상이 무시된다는 것이다. 데이터가 이미지일 경우를 예를 들어보면, 이미지는 세로/가로/채널(색상)으로 구성된 3차원 데이터이다. 이것을 Fully-Connected Layer에 입력하려면, 1차원 데이터로 변환해주어야 한다. 이 때문에 dimension을 압축하는 과정에서 데이터가 가지고 있는 본질적인 패턴이 무시되어 진다. 하지만, Convolutional Layer은 3차원 데이터를 그대로 입력받으며, 출력도 동일하게 3차원 데이터로 전달한다. 따라서 이미지처럼 데이터에 형상이 존재한다면, CNN을 사용하기 적합하다.
- CNN에서 input/output 데이터를 feature map이라고도 한다.
- Convolutional Layer에서는 convolution이 이루어지고, 이미지 data에서는 filter를 의미한다.(filter를 kernel이라고 부르기도 한다.)

![Untitled](/images/2022/BttmDL1_Chap7/0.jpeg)

- window를 일정 간격으로 이동해가며 input data와 FMA를 통해 output을 만들어 낸다.
- 첫 번째 그림: $1\times2+2\times0+3\times1+0\times0+1\times1+2\times2+3\times1+0\times0+1\times2$

![Untitled](/images/2022/BttmDL1_Chap7/1.jpeg)

- CNN에서 filter의 parameter가 가중치를 의미한다. 또, CNN에서도 bias가 존재한다.
- filter를 거친 뒤에, bias가 더해질 수 있다.

## Padding

![Untitled](/images/2022/BttmDL1_Chap7/2.jpeg)

- convolution하기 전에 input data의 주변 값을 채우는 것을 **padding**이라고 한다.
- padding은 output size를 조절하기 위해 많이 사용된다.

## Stride

![Untitled](/images/2022/BttmDL1_Chap7/3.jpeg)

- filter를 연산에 적용하는 위치의 간격을 **stride**라고 한다.

![Untitled](/images/2022/BttmDL1_Chap7/4.jpeg)

## 3 Dimension Convolution

![Untitled](/images/2022/BttmDL1_Chap7/5.jpeg)

- 3 Dimension Convolution은 각 input channel과 filter channel마다의 convolution을 통해 얻은 값들을 다 더한다.
- 단, input과 filter의 channel 개수가 같아야 한다.

![Untitled](/images/2022/BttmDL1_Chap7/6.jpeg)

- 이 때 filter의 개수를 여러개 정할 수 있으며, filter의 개수에 따라 output도 filter 개수만큼 생성된다.

![Untitled](/images/2022/BttmDL1_Chap7/7.jpeg)

- CNN도 Fully-Connected Layer처럼 bias가 사용된다.

![Untitled](/images/2022/BttmDL1_Chap7/8.jpeg)

- batche당 N개의 데이터가 있다고 한다면, Conv Layer에서는 N회 분의 연산을 한번에 수행하게 된다.

## Pooling Layer

- pooling은 dimension을 줄이는 연산이다.

![Untitled](/images/2022/BttmDL1_Chap7/9.jpeg)

- 해당 예시는 Max Pooling이다.
- 보통 Image Recognition에서는 Max Pooling을 주로 사용한다.
- 특징
    - 학습해야할 parameter가 없다.
    - channel의 개수가 변하지 않는다.
    - input data의 변화에 영향을 적게 받는다.(강건하다)

# LeNet(1998)

- Convolutional Layer와 Pooling Layer의 반복하고, 마지막에 Fully-Connected Layer를 거쳐 output을 만들어낸다.
- 최근의 CNN과 다른 점이 몇 가지있다.
    1. activation func이 sigmoid였지만, 현재는 주로 ReLU를 사용한다.
    2. LeNet은 subsampling을 하여 layer를 거칠 수록 데이터의 크기가 작아지지만, 현재는 max pooling이 주로 사용된다.

# AlexNet(2012)

- LeNet과 동일하게 Convolutional Layer와 Pooling Layer의 반복하고, 마지막에 Fully-Connected Layer를 거쳐 output을 만들어낸다.
- LeNet과 몇 가지 다른 점이 있다.
    1. activation func이 ReLU이다.
    2. Local Response Normalization이라는 국소적 정규화를 실시하는 layer를 이용한다.
    3. dropout을 사용한다.
